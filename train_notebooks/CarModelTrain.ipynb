{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CarModelTrain.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yqnv5ml1lw-g",
        "outputId": "33505c75-f611-4c94-c011-589f830d058f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/drive/MyDrive/Cars/car_models_data\""
      ],
      "metadata": {
        "id": "FTAjIKkZl5Kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "def get_model_name(m_string):\n",
        "  return m_string.split(\"/\")[-2].split(\" \")[0]\n",
        "\n",
        "classes_txt_file = \"/content/drive/MyDrive/Cars/trunc_car_models.txt\"\n",
        "classes = []\n",
        "with open(classes_txt_file) as f:\n",
        "  for line in f.readlines():\n",
        "    classes.append(line.strip())\n",
        "\n",
        "classes = list(sorted(classes))\n",
        "\n",
        "# num_classes = 5\n",
        "# classes = classes[0:3]\n",
        "print(classes)\n",
        "\n",
        "res_dict = {}\n",
        "all_img_paths = glob.glob(os.path.join(dataset_path, \"car_data\", \"car_data\", \"train\", \"*\", \"*\"))\n",
        "filtered_classes = classes #[]\n",
        "all_img_paths = list(filter(lambda x: get_model_name(x) in filtered_classes, all_img_paths))\n",
        "\n",
        "for img_path in all_img_paths:\n",
        "  model_name = get_model_name(img_path)\n",
        "\n",
        "  if model_name not in res_dict:\n",
        "    res_dict[model_name] = []\n",
        "  res_dict[model_name].append(img_path)\n",
        "\n",
        "\n",
        "\n",
        "# print(len(filtered_classes))\n",
        "# print(res_dict)\n",
        "# for key, value in res_dict.items():\n",
        "#   if len(value) > 100:\n",
        "#     filtered_classes.append(key)\n",
        "\n",
        "# all_img_paths = glob.glob(os.path.join(dataset_path, \"car_data\", \"car_data\", \"train\", \"*\", \"*\"))\n",
        "\n",
        "# for value in res_dict.items():\n",
        "#   print(value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEYlL93AEg2L",
        "outputId": "aedc7fa4-8a66-4ed9-8e8a-524f2ac083b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Audi', 'BMW', 'Chevrolet', 'Daewoo', 'FIAT', 'Ford', 'Honda', 'Hyundai', 'Infiniti', 'Mazda', 'Mercedes-Benz', 'Mitsubishi', 'Nissan', 'Tesla', 'Toyota', 'Volkswagen', 'Volvo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_paths = [] \n",
        "val_img_paths = []\n",
        "for key, value in res_dict.items():\n",
        "    np.random.shuffle(value)\n",
        "    len_img_paths = len(value)\n",
        "    train_img_paths += value[0:int(len_img_paths * 0.7)]\n",
        "    val_img_paths += value[int(len_img_paths * 0.7):]\n",
        "\n",
        "# should be equal\n",
        "print(len(filtered_classes))\n",
        "print(len(set(list(map(lambda x: get_model_name(x), train_img_paths)))))\n",
        "print(len(set(list(map(lambda x: get_model_name(x), val_img_paths)))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRsmtFoTC20L",
        "outputId": "c1ec584d-978b-4d74-98de-7c0b08e7f1c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17\n",
            "17\n",
            "17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf  \n",
        "import cv2\n",
        "from PIL import Image, ImageOps\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "\n",
        "# https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
        "\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "\n",
        "  def __init__(self, paths, classes):\n",
        "    self.batch_size = 8\n",
        "    self.dim = (224, 224, 3)\n",
        "    self.img_paths = paths #\n",
        "\n",
        "    self.classes = classes\n",
        "    \n",
        "    self.temp_img = None\n",
        "\n",
        "    # classses_ids = np.arange(0, len(self.unique_classes))\n",
        "    \n",
        "    self.indexes = np.arange(len(self.img_paths))\n",
        "\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    print(\"on_epoch_end\")\n",
        "    np.random.shuffle(self.indexes)\n",
        "\n",
        "  def get_model_name(self, m_string):\n",
        "    return m_string.split(\"/\")[-2].split(\" \")[0]\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(len(self.img_paths) / self.batch_size)\n",
        "\n",
        "  def __data_generation(self, img_paths):\n",
        "      'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "      # Initialization\n",
        "      X = np.empty((self.batch_size, *self.dim))\n",
        "      y = np.empty((self.batch_size), dtype=int)\n",
        "\n",
        "      # Generate data\n",
        "      for i, img_path in enumerate(img_paths):\n",
        "          # Store sample\n",
        "          img = Image.open(img_path)#.convert('L')\n",
        "          img = self.resize_with_padding(img, (224, 224))\n",
        "          # img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "          if np.asarray(img).ndim == 3:\n",
        "            self.temp_img = img_path #self.resize_with_padding(img, (224, 224))\n",
        "\n",
        "\n",
        "          if np.asarray(img).ndim == 2:\n",
        "            img_path = self.temp_img\n",
        "            img = Image.open(img_path) \n",
        "            img = self.resize_with_padding(img, (224, 224))\n",
        "\n",
        "          X[i,] = preprocess_input(np.asarray(img)) \n",
        "          y[i] = self.classes.index(self.get_model_name(img_path))\n",
        "\n",
        "      return X, y #tf.keras.utils.to_categorical(y, num_classes=len(self.classes))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        img_paths = [self.img_paths[k] for k in indexes]\n",
        "        item = self.__data_generation(img_paths)\n",
        "\n",
        "        return item\n",
        "\n",
        "\n",
        "  def padding(self, img, expected_size):\n",
        "      desired_size = expected_size\n",
        "      delta_width = desired_size - img.size[0]\n",
        "      delta_height = desired_size - img.size[1]\n",
        "      pad_width = delta_width // 2\n",
        "      pad_height = delta_height // 2\n",
        "      padding = (pad_width, pad_height, delta_width - pad_width, delta_height - pad_height)\n",
        "      return ImageOps.expand(img, padding)\n",
        "\n",
        "\n",
        "  def resize_with_padding(self, img, expected_size):\n",
        "      img.thumbnail((expected_size[0], expected_size[1]))\n",
        "      # print(img.size)\n",
        "      delta_width = expected_size[0] - img.size[0]\n",
        "      delta_height = expected_size[1] - img.size[1]\n",
        "      pad_width = delta_width // 2\n",
        "      pad_height = delta_height // 2\n",
        "      padding = (pad_width, pad_height, delta_width - pad_width, delta_height - pad_height)\n",
        "      return ImageOps.expand(img, padding)"
      ],
      "metadata": {
        "id": "OEICu_PKmKJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(filtered_classes))\n",
        "train_dg = DataGenerator(train_img_paths, filtered_classes)\n",
        "val_dg = DataGenerator(val_img_paths, filtered_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TN9Fb4JJmMAY",
        "outputId": "eb3179e9-176b-4546-8d6d-acbfdb57ddca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "# for item in train_dg:\n",
        "#   print(item[1])\n",
        "\n",
        "item = train_dg[7]#[0]\n",
        "print(item[0].shape)\n",
        "cv2_imshow(item[0][0])"
      ],
      "metadata": {
        "id": "RREeL-UCnN65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "f9ab9f78-bcb3-4254-dd6a-98e1029fb2ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 224, 224, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=224x224 at 0x7F5009E48690>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAEPklEQVR4nO3dbXOyOhAA0Owz9///5b0feA0ERSyI7TnjWIUIIVk2gU5tKQAAAAAAAAAAAAAAAAAAAAAAAAyivTTay/+m3Fh+hzbKnGo39dqwNKJkjtXMrW4dN/K0wFaZwwXmy5v+PV7NljtE51/w36cr8AXE4gfJoG3zoad7nZlPx6O/5oIGqbLDBVPPa/r4/QN5VM+IUq+93ZR9nIOWkuX5HPS9nTw6/D1l1oXnDPFtETG11+ziY3r+Gnn2JOXUpGOIb6saPbN/dCK6Hr9Z2pzMLphP3EsM7VCiRMRJw8hZ292SmzdtJvHg3biZEqXOZbEsUK+JakWU7F8fPvlj+mzUT63q5uJ9V/T9tm9kr9kMZDHE9zuMrSoNG1h8ri4S/evuDlbMOyFWzbmaDVV7yZJRohuVokSJ7Pp1/pGbD/HHOnBf0HWNMbT3STv5sGhNSBZjfmN9Ltq9Gk7m67p4H1avm+Tp4D+eJ91O+4wcw9by+gDd06/PE8s8K+wuOyudGfuy+S5ZJeiHlZ9S22mWSazaaRVKURWIaVhpjS2za61Sskt878w+x9bvwzFLZj/1j3HR9Rk0NvomN16vS9fXz9XyKRu2hv7GWBg/MMGPfhh7GJoXxOVUoelQ66bIiMh6UQylc5HtxqlAFcvj2njp/K7z7FIjxeeUo6L5e6cfmZgu+r7b5uGAGKuUjbO7Nf1ZXIm3tnjB9fjGUT/q4Jd+E9j8/K7jOjy3Oc26vyLiuqv43L6ieXVLe5cN1yJbFXqnEpWNnXQt3gqy/LHpxWF5r+gspWTmIjFmLmfDvTMy6LDp19rlSRbc/Ng4NNXp5JKsuapLlG4u8eKxH93fB47xPP9idgsrtmaIg/Gsy/kjZ8szsz45l7H+YtMdnBJkPYQt7rSfLbp7hMOcpAxJdGvnsxaaeiMOjDO3vTN7XOOQIlb9WvLZlLg9o7kmZfwRvyszniOmW1UfrskuH6nk3X4v/+V+oDXnPXLuKS4hL7xyOxgAAAAAAAAAAAAAAAAAAAAAAAAAAACAi/hC9YOi+rH/E1lK9Y91pn/pNhb4/P9QuhEB2jaLG/9bAwAAAAAAAAAAAAAAAAD4Ab/2rzoj+kNLf5T5zX5JgI7h2FkHZUTx9+ZcIXafU7OoHb4aIfoH3+L2fTVF0yz9ZbcmFplyY1ivttCVyRy/msEU4NZuF6DRPcUQOcsK1u8bQ3k91pfSHNerQjGUyPGJu7hLgPaBlX1ERSmlS5D98o2abmTQVZkoqwwcw5IQlDd2aYBGN6zOBtWI17/5aB2FmQ+2Jf54IgbD++X6Vzb16BLnLsMB3yXqwFpdW+/4+PBoruUX+1j/zr/fcOJWJQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANzY/wsZlTu7LiiAAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "def get_simple_model(input_shape, num_classes):\n",
        "    print(\"num_classes\", num_classes)\n",
        "      # Input layer\n",
        "    input_layer = tf.keras.Input(shape=input_shape, name=\"Input\")\n",
        "    input_layer = tf.keras.layers.Rescaling(1./255)(input_layer)\n",
        "    input_layer = tf.keras.layers.RandomFlip(\"horizontal_and_vertical\")(input_layer)\n",
        "    input_layer = tf.keras.layers.RandomRotation(0.2)(input_layer)\n",
        "    # Base layer\n",
        "    base_model = tf.keras.layers.Conv2D(16, 3, padding='same', activation=LeakyReLU(alpha=0.001))(input_layer)\n",
        "    base_model = tf.keras.layers.MaxPooling2D()(base_model)\n",
        "    base_model = tf.keras.layers.Conv2D(32, 3, padding='same', activation=LeakyReLU(alpha=0.001))(base_model)\n",
        "    base_model = tf.keras.layers.MaxPooling2D()(base_model)\n",
        "    base_model = tf.keras.layers.Conv2D(64, 3, padding='same', activation=LeakyReLU(alpha=0.001))(base_model)\n",
        "    base_model = tf.keras.layers.MaxPooling2D()(base_model)\n",
        "    base_model = tf.keras.layers.Conv2D(128, 3, padding='same', activation=LeakyReLU(alpha=0.001))(base_model)\n",
        "    base_model = tf.keras.layers.MaxPooling2D()(base_model)\n",
        "    base_model = tf.keras.layers.Flatten()(base_model)\n",
        "    # Output layer\n",
        "    output_layer = tf.keras.layers.Dense(num_classes, name=\"Output\")(base_model)\n",
        "    # Compile model\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_model(input_shape, num_classes):\n",
        "  base_model =  tf.keras.applications.mobilenet_v2.MobileNetV2(weights='imagenet', include_top=False) # replace with simpler CNN # skip some classes # grayscale #augmentation \n",
        "  for index, layer in enumerate(base_model.layers):\n",
        "    # print(index > len(base_model.layers) * 0.95)\n",
        "    layer.trainable = index > len(base_model.layers) * 0.7\n",
        "\n",
        "\n",
        "  inputs = tf.keras.Input(shape=input_shape)\n",
        "  x = base_model(inputs)\n",
        "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "  # A Dense classifier with a single unit (binary classification)\n",
        "  outputs = tf.keras.layers.Dense(num_classes)(x)\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "Tlb28VPm9pfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3g9mwOA0r8DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model((224, 224, 3), len(filtered_classes))\n",
        "# model5.load_weights(\"/content/drive/MyDrive/Cars/checkpoints/cp.ckpt\")\n",
        "# model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0001), metrics=['accuracy'])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "checkpoint_filepath = \"/content/drive/MyDrive/Cars/checkpoints/MobileNetV2.ckpt\"\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_sparse_categorical_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "model.fit_generator(generator=train_dg,\n",
        "                              steps_per_epoch=1000,\n",
        "                              epochs=15,\n",
        "                              verbose=1,\n",
        "                              callbacks=[model_checkpoint_callback],\n",
        "                              workers=6,\n",
        "                              use_multiprocessing=True,\n",
        "                              # max_queue_size=config[\"max-queue-size\"],\n",
        "                              validation_data=val_dg)"
      ],
      "metadata": {
        "id": "OuieSA5f92YA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2316fa90-4a7a-4bef-d164-a5d4062bb03b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n",
            "9420800/9406464 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            " 375/1000 [==========>...................] - ETA: 4:34 - loss: 2.6461 - sparse_categorical_accuracy: 0.1553on_epoch_end\n",
            " 750/1000 [=====================>........] - ETA: 1:04 - loss: 2.4446 - sparse_categorical_accuracy: 0.2272on_epoch_end\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 2.3116 - sparse_categorical_accuracy: 0.2747on_epoch_end\n",
            "1000/1000 [==============================] - 307s 292ms/step - loss: 2.3116 - sparse_categorical_accuracy: 0.2747 - val_loss: 10.1753 - val_sparse_categorical_accuracy: 0.3048\n",
            "on_epoch_end\n",
            "Epoch 2/15\n",
            " 375/1000 [==========>...................] - ETA: 48s - loss: 1.6229 - sparse_categorical_accuracy: 0.5090on_epoch_end\n",
            " 749/1000 [=====================>........] - ETA: 19s - loss: 1.4958 - sparse_categorical_accuracy: 0.5419on_epoch_end\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 1.4111 - sparse_categorical_accuracy: 0.5673on_epoch_end\n",
            "1000/1000 [==============================] - 93s 92ms/step - loss: 1.4111 - sparse_categorical_accuracy: 0.5673 - val_loss: 14.9227 - val_sparse_categorical_accuracy: 0.0810\n",
            "on_epoch_end\n",
            "Epoch 3/15\n",
            " 375/1000 [==========>...................] - ETA: 48s - loss: 0.9321 - sparse_categorical_accuracy: 0.7013on_epoch_end\n",
            " 750/1000 [=====================>........] - ETA: 19s - loss: 0.8379 - sparse_categorical_accuracy: 0.7310on_epoch_end\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.7565 - sparse_categorical_accuracy: 0.7579on_epoch_end\n",
            "1000/1000 [==============================] - 92s 92ms/step - loss: 0.7565 - sparse_categorical_accuracy: 0.7579 - val_loss: 9.8504 - val_sparse_categorical_accuracy: 0.2384\n",
            "on_epoch_end\n",
            "Epoch 4/15\n",
            " 374/1000 [==========>...................] - ETA: 48s - loss: 0.4414 - sparse_categorical_accuracy: 0.8516on_epoch_end\n",
            " 748/1000 [=====================>........] - ETA: 19s - loss: 0.4069 - sparse_categorical_accuracy: 0.8648on_epoch_end\n",
            " 999/1000 [============================>.] - ETA: 0s - loss: 0.3728 - sparse_categorical_accuracy: 0.8764on_epoch_end\n",
            "on_epoch_end\n",
            "1000/1000 [==============================] - 94s 93ms/step - loss: 0.3726 - sparse_categorical_accuracy: 0.8765 - val_loss: 9.1621 - val_sparse_categorical_accuracy: 0.3164\n",
            "on_epoch_end\n",
            "Epoch 5/15\n",
            " 375/1000 [==========>...................] - ETA: 48s - loss: 0.2749 - sparse_categorical_accuracy: 0.9053on_epoch_end\n",
            " 749/1000 [=====================>........] - ETA: 19s - loss: 0.2531 - sparse_categorical_accuracy: 0.9161on_epoch_end\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2279 - sparse_categorical_accuracy: 0.9254on_epoch_end\n",
            "1000/1000 [==============================] - 94s 93ms/step - loss: 0.2279 - sparse_categorical_accuracy: 0.9254 - val_loss: 2.4508 - val_sparse_categorical_accuracy: 0.6173\n",
            "on_epoch_end\n",
            "Epoch 6/15\n",
            " 374/1000 [==========>...................] - ETA: 49s - loss: 0.1557 - sparse_categorical_accuracy: 0.9525on_epoch_end\n",
            " 749/1000 [=====================>........] - ETA: 20s - loss: 0.1686 - sparse_categorical_accuracy: 0.9461on_epoch_end\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.1615 - sparse_categorical_accuracy: 0.9484on_epoch_end\n",
            "1000/1000 [==============================] - 94s 93ms/step - loss: 0.1615 - sparse_categorical_accuracy: 0.9484 - val_loss: 5.7704 - val_sparse_categorical_accuracy: 0.4252\n",
            "on_epoch_end\n",
            "Epoch 7/15\n",
            " 374/1000 [==========>...................] - ETA: 48s - loss: 0.1304 - sparse_categorical_accuracy: 0.9582on_epoch_end\n",
            " 749/1000 [=====================>........] - ETA: 19s - loss: 0.1055 - sparse_categorical_accuracy: 0.9670on_epoch_end\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.1108 - sparse_categorical_accuracy: 0.9657on_epoch_end\n",
            "on_epoch_end\n",
            "1000/1000 [==============================] - 94s 93ms/step - loss: 0.1108 - sparse_categorical_accuracy: 0.9657 - val_loss: 2.9809 - val_sparse_categorical_accuracy: 0.5980\n",
            "on_epoch_end\n",
            "Epoch 8/15\n",
            " 374/1000 [==========>...................] - ETA: 48s - loss: 0.1494 - sparse_categorical_accuracy: 0.9505on_epoch_end\n",
            " 750/1000 [=====================>........] - ETA: 19s - loss: 0.1372 - sparse_categorical_accuracy: 0.9552on_epoch_end\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.1238 - sparse_categorical_accuracy: 0.9600on_epoch_end\n",
            "1000/1000 [==============================] - 93s 92ms/step - loss: 0.1238 - sparse_categorical_accuracy: 0.9600 - val_loss: 2.3876 - val_sparse_categorical_accuracy: 0.6752\n",
            "on_epoch_end\n",
            "Epoch 9/15\n",
            " 375/1000 [==========>...................] - ETA: 48s - loss: 0.0538 - sparse_categorical_accuracy: 0.9813on_epoch_end\n",
            " 749/1000 [=====================>........] - ETA: 19s - loss: 0.0794 - sparse_categorical_accuracy: 0.9756on_epoch_end\n",
            " 999/1000 [============================>.] - ETA: 0s - loss: 0.0846 - sparse_categorical_accuracy: 0.9743on_epoch_end\n",
            "1000/1000 [==============================] - 93s 92ms/step - loss: 0.0845 - sparse_categorical_accuracy: 0.9744 - val_loss: 2.6336 - val_sparse_categorical_accuracy: 0.6265\n",
            "on_epoch_end\n",
            "Epoch 10/15\n",
            " 373/1000 [==========>...................] - ETA: 48s - loss: 0.0981 - sparse_categorical_accuracy: 0.9675on_epoch_end\n",
            " 750/1000 [=====================>........] - ETA: 19s - loss: 0.0945 - sparse_categorical_accuracy: 0.9690on_epoch_end\n",
            " 999/1000 [============================>.] - ETA: 0s - loss: 0.0854 - sparse_categorical_accuracy: 0.9722on_epoch_end\n",
            "1000/1000 [==============================] - 93s 92ms/step - loss: 0.0853 - sparse_categorical_accuracy: 0.9722 - val_loss: 3.1774 - val_sparse_categorical_accuracy: 0.6211\n",
            "on_epoch_end\n",
            "Epoch 11/15\n",
            " 375/1000 [==========>...................] - ETA: 48s - loss: 0.1096 - sparse_categorical_accuracy: 0.9623on_epoch_end\n",
            " 750/1000 [=====================>........] - ETA: 19s - loss: 0.1060 - sparse_categorical_accuracy: 0.9653on_epoch_end\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.0951 - sparse_categorical_accuracy: 0.9695on_epoch_end\n",
            "1000/1000 [==============================] - 94s 92ms/step - loss: 0.0951 - sparse_categorical_accuracy: 0.9695 - val_loss: 3.3426 - val_sparse_categorical_accuracy: 0.6157\n",
            "on_epoch_end\n",
            "Epoch 12/15\n",
            " 375/1000 [==========>...................] - ETA: 48s - loss: 0.0749 - sparse_categorical_accuracy: 0.9773on_epoch_end\n",
            " 749/1000 [=====================>........] - ETA: 19s - loss: 0.0736 - sparse_categorical_accuracy: 0.9755on_epoch_end\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.0697 - sparse_categorical_accuracy: 0.9769on_epoch_end\n",
            "1000/1000 [==============================] - 94s 93ms/step - loss: 0.0697 - sparse_categorical_accuracy: 0.9769 - val_loss: 2.4197 - val_sparse_categorical_accuracy: 0.6736\n",
            "on_epoch_end\n",
            "Epoch 13/15\n",
            " 375/1000 [==========>...................] - ETA: 49s - loss: 0.0820 - sparse_categorical_accuracy: 0.9740on_epoch_end\n",
            " 748/1000 [=====================>........] - ETA: 20s - loss: 0.0679 - sparse_categorical_accuracy: 0.9784on_epoch_end\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.0690 - sparse_categorical_accuracy: 0.9784on_epoch_end\n",
            "1000/1000 [==============================] - 94s 93ms/step - loss: 0.0690 - sparse_categorical_accuracy: 0.9784 - val_loss: 4.5687 - val_sparse_categorical_accuracy: 0.4946\n",
            "on_epoch_end\n",
            "Epoch 14/15\n",
            " 374/1000 [==========>...................] - ETA: 49s - loss: 0.0629 - sparse_categorical_accuracy: 0.9816on_epoch_end\n",
            " 750/1000 [=====================>........] - ETA: 19s - loss: 0.0693 - sparse_categorical_accuracy: 0.9795on_epoch_end\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.0593 - sparse_categorical_accuracy: 0.9822on_epoch_end\n",
            "1000/1000 [==============================] - 94s 93ms/step - loss: 0.0593 - sparse_categorical_accuracy: 0.9822 - val_loss: 2.3944 - val_sparse_categorical_accuracy: 0.6404\n",
            "on_epoch_end\n",
            "Epoch 15/15\n",
            " 375/1000 [==========>...................] - ETA: 49s - loss: 0.0501 - sparse_categorical_accuracy: 0.9840on_epoch_end\n",
            " 750/1000 [=====================>........] - ETA: 19s - loss: 0.0594 - sparse_categorical_accuracy: 0.9815on_epoch_end\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.0667 - sparse_categorical_accuracy: 0.9797on_epoch_end\n",
            "1000/1000 [==============================] - 93s 92ms/step - loss: 0.0667 - sparse_categorical_accuracy: 0.9797 - val_loss: 3.9692 - val_sparse_categorical_accuracy: 0.5779\n",
            "on_epoch_end\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f19066abc50>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "# VALIDATION !\n",
        "total_val_loss = 0\n",
        "batch_size = 8\n",
        "y_total_batch_val = np.empty((len(val_dg), 8))\n",
        "y_pred_batch_val = np.empty((len(val_dg), 8))\n",
        "# model = get_model((224, 224, 3), len(filtered_classes))\n",
        "# model.load_weights(\"/content/drive/MyDrive/Cars/checkpoints/resnet.ckpt/resnetcp.ckpt\")\n",
        "\n",
        "\n",
        "for step, (x_batch_val, y_batch_val) in enumerate(val_dg):\n",
        "    strt_index = step * batch_size\n",
        "    y_total_batch_val[strt_index:strt_index+batch_size] = y_batch_val\n",
        "    # print(x_batch_val.shape)\n",
        "    val_logits = model(x_batch_val, training=False)\n",
        "\n",
        "    y_pred_batch_val[strt_index:strt_index+8] = np.argmax(val_logits, axis=1)\n",
        "\n",
        "print(classification_report(y_true=y_total_batch_val.flatten(),\n",
        "                            y_pred=y_pred_batch_val.flatten(),\n",
        "                            labels=np.arange(len(filtered_classes)),\n",
        "                            target_names = filtered_classes, zero_division=0))\n",
        "\n",
        "print(accuracy_score(y_true=y_total_batch_val.flatten(), y_pred=y_pred_batch_val.flatten()))\n",
        "# model.save(\"{}/weights/{}/ep_{}\".format(BASE_SAVE_FOLDER, \"temp\", epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knU18BHmWqRa",
        "outputId": "d098b5af-c180-476b-9a2c-d026c364d4ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "         Audi       0.55      0.87      0.67       180\n",
            "          BMW       0.40      0.68      0.50       124\n",
            "    Chevrolet       0.77      0.72      0.74       258\n",
            "       Daewoo       0.00      0.00      0.00        40\n",
            "         FIAT       1.00      0.50      0.67        32\n",
            "         Ford       1.00      0.53      0.69       140\n",
            "        Honda       1.00      0.20      0.33        80\n",
            "      Hyundai       0.62      0.57      0.59       112\n",
            "     Infiniti       0.25      1.00      0.40        24\n",
            "        Mazda       0.00      0.00      0.00        16\n",
            "Mercedes-Benz       1.00      0.13      0.24       120\n",
            "   Mitsubishi       0.15      1.00      0.27        16\n",
            "       Nissan       0.38      0.38      0.38        26\n",
            "        Tesla       0.20      0.50      0.29        16\n",
            "       Toyota       0.32      0.40      0.36        40\n",
            "   Volkswagen       1.00      0.33      0.50        24\n",
            "        Volvo       1.00      0.17      0.29        48\n",
            "\n",
            "     accuracy                           0.54      1296\n",
            "    macro avg       0.57      0.47      0.41      1296\n",
            " weighted avg       0.69      0.54      0.53      1296\n",
            "\n",
            "0.5416666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classes)\n",
        "# for item in train_dg:\n",
        "#   print(item[1].shape)\n",
        "#   preds = model(item[0], training=False)\n",
        "#   print(preds.shape)\n",
        "#   print(tf.keras.losses.CategoricalCrossentropy(from_logits=True)(preds, item[1]))"
      ],
      "metadata": {
        "id": "bRR0fao9-UO2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce066181-6959-4fd1-fbe4-52ece1fbd288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Audi', 'BMW', 'Chevrolet', 'Daewoo', 'FIAT']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uDtckDKM-sO5",
        "outputId": "55a5b66c-f5cd-4d19-916e-aa6da3f063eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            " 10/600 [..............................] - ETA: 10s - loss: nan - accuracy: 0.0812"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnknownError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-0136c05cb5ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                               \u001b[0;31m# max_queue_size=config[\"max-queue-size\"],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                               validation_data=val_dg)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2221\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2222\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2223\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2225\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_not_generate_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\n2 root error(s) found.\n  (0) UNKNOWN:  IndexError: index 4575657221408423936 is out of bounds for axis 1 with size 17\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1004, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\", line 830, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/usr/local/lib/python3.7/dist-packages/keras/utils/data_utils.py\", line 785, in get\n    raise e\n\n  File \"/usr/local/lib/python3.7/dist-packages/keras/utils/data_utils.py\", line 776, in get\n    inputs = self.queue.get(block=True, timeout=5).get()\n\n  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 657, in get\n    raise self._value\n\n  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n\n  File \"/usr/local/lib/python3.7/dist-packages/keras/utils/data_utils.py\", line 566, in get_index\n    return _SHARED_SEQUENCES[uid][i]\n\n  File \"<ipython-input-28-8af6dad1b83b>\", line 67, in __getitem__\n    item = self.__data_generation(img_paths)\n\n  File \"<ipython-input-28-8af6dad1b83b>\", line 62, in __data_generation\n    return X, tf.keras.utils.to_categorical(y, num_classes=len(self.unique_classes))\n\n  File \"/usr/local/lib/python3.7/dist-packages/keras/utils/np_utils.py\", line 71, in to_categorical\n    categorical[np.arange(n), y] = 1\n\nIndexError: index 4575657221408423936 is out of bounds for axis 1 with size 17\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_7]]\n  (1) UNKNOWN:  IndexError: index 4575657221408423936 is out of bounds for axis 1 with size 17\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1004, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\", line 830, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/usr/local/lib/python3.7/dist-packages/keras/utils/data_utils.py\", line 785, in get\n    raise e\n\n  File \"/usr/local/lib/python3.7/dist-packages/keras/utils/data_utils.py\", line 776, in get\n    inputs = self.queue.get(block=True, timeout=5).get()\n\n  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 657, in get\n    raise self._value\n\n  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n\n  File \"/usr/local/lib/python3.7/dist-packages/keras/utils/data_utils.py\", line 566, in get_index\n    return _SHARED_SEQUENCES[uid][i]\n\n  File \"<ipython-input-28-8af6dad1b83b>\", line 67, in __getitem__\n    item = self.__data_generation(img_paths)\n\n  File \"<ipython-input-28-8af6dad1b83b>\", line 62, in __data_generation\n    return X, tf.keras.utils.to_categorical(y, num_classes=len(self.unique_classes))\n\n  File \"/usr/local/lib/python3.7/dist-packages/keras/utils/np_utils.py\", line 71, in to_categorical\n    categorical[np.arange(n), y] = 1\n\nIndexError: index 4575657221408423936 is out of bounds for axis 1 with size 17\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_28510]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "item = train_dg[0]"
      ],
      "metadata": {
        "id": "IRLnCJnZKen5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(item[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbCJxTY8KypN",
        "outputId": "bc97dbe0-10f0-4b76-88e2-7fd2e1d61b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 \n",
        "import tensorflow as tf\n",
        "from google.colab.patches import cv2_imshow\n",
        "import keras\n",
        "\n",
        "img_x = cv2.imread(\"/content/drive/MyDrive/Cars/car_models_data/car_data/car_data/train/AM General Hummer SUV 2000/00163.jpg\")\n",
        "# img = cv2.resize(img, (224, 224)) #self.resize_with_padding(img, (224, 224))\n",
        "# img = np.expand_dims(img, axis=0)\n",
        "img_2 = keras.applications.mobilenet.preprocess_input(img_x)\n",
        "print(img_2)\n",
        "cv2_imshow(img_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "j9SwPZgVK899",
        "outputId": "33f668a1-72a0-47b7-e96d-d50a0b41e75d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  ...\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  ...\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  ...\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  ...\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  ...\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  ...\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=700x525 at 0x7F429CB8B890>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7hQkT5P_kJeQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}